1.  Print only words with a length > 5 characters. Submit the pyspark code
>>>from pyspark import SparkContext
>>>from pyspark.streaming import StreamingContext
>>>ssc = StreamingContext(sc, 1)
>>>lines= ssc.textFileStream("file:///tmp/datastreams")
>>>uclines = lines.filter(lambda word: len(word) > 5)
>>>uclines.pprint()
>>>ssc.start()

2.  Change the code so that you save the venue components to a text file. Submit you code.
>>>from pyspark import SparkContext
>>>from pyspark.streaming import StreamingContext
>>>import json
>>>ssc = StreamingContext(sc, 10)
>>>lines = ssc.textFileStream("file:///tmp/datastreams")
>>>slines = lines.flatMap(lambda x: [ j['venue'] for j in json.loads('['+x+']') if 'venue' in j] )
>>>cnt=slines.count()
>>>cnt.pprint()
>>>slines.pprint()
>>>slines.saveAsTextFile('/data/w205/Q2.txt')

3.  In a previous module in this class you learnt about streams, bustiness and kafka. Describe how you would solve a situation where (1) you have a very busty stream where you spark streaming process may not always be able to keep up with the data it receives, meaning it the time it takes to process takes longer than the batch interval. (2) Like other programs stream processing programs need to be updated. Describe the implications of updating this simple processing program. What side effects can it have? How could you potentially handle them?
By adding extra nodes, you could better keep up with the data if it takes tlonger to process than the batch interval.
Updating this simple processing program would result in loss of data during the time that the program is down for update.  This can be remedied by having redundant servers. One can be updating while the other is still processing data and vice versa.

4.  Also explain what the difference is between having 10 sec batches with a 30 sec sliding window and a 30 second batch length.
For 10 second batches with 30 sec sliding windos, the data is collected for 10 secs every 30 secs.  Then, it reports the last 3 batches of 10 second collections.  A 30 sec batch length includes data for only that batch.